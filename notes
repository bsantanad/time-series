===========
time series
===========

notes from, time series forecasting by Marco Peixeiro

intro
=====

time series: set of points ordered in time. usually equally spaced in time.

has 3 components:

- trend:
    the slow-moving changes in a time series. like trying to draw a line
    through most the data points to show the general direction of a time series

- seasonal component:
    cycle that occurs over a fixed period of time.

- residuals:
    what can not be explained by the trend or the seasonal components. we can
    think of it as, adding the trend ans seasonal graphs together and comparing
    the value at each point

    usually corresponds to random errors, white noise and so on.

    they represent info the model can not predict since it is random.


visualizing these components is known as _decomposition_


brid's-eye view of time series forecasting
------------------------------------------

forecasting: predicting the future using historical data and knowledge of
future events that might affect our forecasts.


1. set a goal
2. determine what must be forecast to achieve our goal
3. set the horizon of the forecast (horizon: period you want to predict the
data on)
4. gather the data
5. *develop a forecasting model*
6. deploy to production
7. monitor
8. collect new data
9. repeat step 5


diff between time series forecasting and other regressions tasks
----------------------------------------------------------------

- time series have an order
  - time series are indexed by time, order must be kept. on the other hand, say
    you want to predict revenue based on ad spend, it does not matter when
    certain amount was spent on ads.

-  time series do not have features
  - it is common to only have two columns, time and the data itself, it does
    not have categories and so on


baseline model
==============

a baseline model is a trivial solution to our problem. it uses heuristics or
simple statistics to generate predictions.

The baseline model is the simplest solution you can think of.

In the context of time series, simple static we can use to build baseline is
the arithmetic mean.

something like saying:

>>> the average EPS between 1960 and 1979 was $4.31. Therefore, I expect the
>>> EPS over the next four quarters of 1980 to be equal to $4.31 per quarter

other way is to naively forecast the last recorded data point.

>>> if the EPS is $0.71 for this quarter, then the EPS will also be $0.71 for
>>> next quarter

or if we see a cyclical pattern:

>> if the EPS is $14.04 for the first quarter of 1979, then the EPS for the
>> first quarter of 1980 will also be $14.04

when we define a baseline we need to calculate an error metric in order to
evaluate the performance of our forecasts on the test set

- mean absolute percentage error (MAPE):
    measure prediction accuracy, independent of the scale of our data

check the notebook: ch2/baseline.ipynb for more details


random walk
===========

>>> a random walk is a series whose first difference is stationary and
>>> uncorrelated.
>>>
>>> this means that the process moves completely random.

a random walk is a process in which there is an equal chance of going up or
down by a random number. Usually observed in financial and economic data.

Normally they also have long periods of positive or negative trends, but sudden
changes in direction.

a random walks is expressed by:
    the previous value plus white noise and some constant


how to find out if we have a random walk?
-----------------------------------------

1. gather data
2. is it stationary? if not make it (transformations)
3. plot ACF
4. is there autocorrelation?
    - no, random walk
    - yes, not a random walk

let's break down the concepts


stationarity
------------

>>> a stationary time series is one whose statistical properties do not change
>>> over time.
>>>
>>> It has constant mean, variance, and autocorrelation, and these properties
>>> are independent of time.

many forecasting models assume stationarity. moving average model,
autoregressive model, and autoregressive moving average model all assume
stationary.

we rarely see this stationary time series, this is when ARIMA and SARIMA come
into play.

There are several way to transform a time series to become stationary, the
simplest one is differencing. Calculate the changes from one step to another.
This transformation helps stabilize the mean. Which reduces the trend and
seasonality effects.

Differencing: stabilize the mean
Apply log function: stabilize the variance

make sure to inverse transform the data after finishing the model.

- how to test for stationarity?
Augmented Dickey-Fuller (ADF) test, if the p value is less than 0.05 we can say
the series is stationary

a stationary series has constant properties over time, mean and variance are
not a function of time. the mean of stationary process should be flat over time
the mean of non-stationary process should vary


autocorrelation function
------------------------

once the time series become stationary, we plot the autocorrelation function
(acf). It is a great way to understand what type of process you are analyzing.

correlation measures the extent of a linear relationship between two
variables.

>>> the autocorrelation function measures the linear relationship between
>>> lagged values of a time series. ACF measures the correlation of the time
>>> series with itself

in the presence of a trend, a plot of the ACF will show coefficients are high
for short lags, and will decrease as the lag increases. If the data is
seasonal, it will also display cyclical patterns.

check ch3/random_walk.ipynb for more details


basically if we are presented with a random walk, we cannot predict anything at
the long term with an statistical model.

we can only use methods like the one we use in the baseline part. (historical
mean, last known value)

nevertheless there is a method that we have not take into account:

the drift method
----------------

>>> States that the value of our forecast is linearly dependent on the timestep
>>>
>>> Therefore, it is equivalent to calculating the slope between the first and
>>> last value of the train set and simply extrapolating this straight line
>>> into the future.

basically getting the slope and calculating the values based on that.

slope = (delta y) / (delta x)
      = (yf - yi) / #timesteps - 1

check ch3/random_walk.ipynb for more details

we cannot forecast long term a random walk.


forecasting next timestep
-------------------------

since the value of a random walk is the past value by some random number, if we
take the last value we will be off just by that random number

this might look deceiving to untrained eyes, because even the MSE is low. But
remember we are just taking the last known value at each timestep and inserting
it in the next.

the conclusion is, if we find a random walk, we cannot use deep learning or
statistical models to predict future values.


moving average process
======================

>>> In a moving average (MA) process, the current value depends linearly on the
>>> mean of the series, the current error term, and past error terms.

It is usually denoted as MA(q) where q is the order (the number of past error
terms that affect the present value).

yt = µ + εt + (θ1 (ε_(t-1)) + (θ2 (ε_(t-2)) + ... + (θq (ε_(t-q))

where:
- µ is the mean
- εt error term
- ε_(t - k)  past error term
- θ magnitud of impact of the past errors

The larger q is, the more past error terms affect the present value.

This means that we need to choose this number carefully in order to train the
data correctly

1. gather data
2. is it stationary? if not make it (transformations)
3. plot ACF
4. is there autocorrelation?
    - no, random walk
5. yes. Do autocorrelation coefficients become abruptly non-significant after
   lag q?
   - yes, MA(q) process
   - no, not MA(q) process

we get q from the ACF plot, when we see that the correlation ends.

see ch4_moving_average/moving_average_order_q.ipynb for more details

Now, say q = 2, meaning we will only take 2 past errors to calc each value. We
can not take this and calculate 50 timesteps into the future. Because we don't
have the 2 past error for future values.

What we can do is called _rolling forecasting_ which basically is iterate the
training, and based on new results obtain values further in the future

say we have 10 steps, and a q of 2. The first iteration we can predict until
12, the second one we will take the new values and train with those so we will
be able to go until 14.

see ch4_moving_average/moving_average_order_q.ipynb for more details

we can use the mean absolute error, it returns the average of the absolute
difference between the predicted and actual values

again, for more details ch4_moving_average/moving_average_order_q.ipynb
there we have an interpretation.

autoregressive process
======================

an autoregressive process establishes that the output variable depends linearly
on its own previous states.

>>> An autoregressive process is a regression of a variable against itself. In
>>> a time series, this means that the present value is linearly dependent on
>>> its past values.

It is defined by AR(p) where p is the order it defines the number of past
values that affect the present value:

yt = C + (φ1 (y_(t-1)) + (φ2 (y_(t-2)) + ... + (φp (y_(t-p)) + εt

where

- φ autoregressive coefficients that measure the strength of the
  relationship between the current value and past values.
- εt error term
- ε_(t - k)  past error term

if p is 1 and φ1 is 1,

yt = C + (φ1 (y_(t-1)) + εt

we have a random walk, so a random walk is a special case of an autoregressive
process.

We can now add steps to our list of time series process:

1. gather data
2. is it stationary? if not make it (transformations)
3. plot ACF
4. is there autocorrelation?
    - no, random walk
5. yes. Do autocorrelation coefficients become abruptly non-significant after
   lag q?
   - yes, MA(q) process
6. no, plot PACF
7. Do coefficients become abruptly non-significant after lag p
   - yes, AR(p) process
   - no, not an AR(p) process

How does the ACF plot looks if we are dealing with an autoregressive process?

check the notebook: ch5_autoregressive/autoregressive.ipynb

>>> when the ACF plot of a stationary process exhibits a pattern of exponential
>>> decay, we probably have an autoregressive process in play.
>>>
>>> we must turn our attention to the partial autocorrelation function (PACF)
>>> plot. to get the order of the autoregression

what does the partial autocorrelation does?

>>> the partial autocorrelation measures the correlation between lagged values
>>> in a time series when we remove the influence of correlated lagged values
>>> in between.

check the notebook: ch5_autoregressive/autoregressive.ipynb
