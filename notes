===========
time series
===========

notes from, time series forecasting by Marco Peixeiro

intro
=====

time series: set of points ordered in time. usually equally spaced in time.

has 3 components:

- trend:
    the slow-moving changes in a time series. like trying to draw a line
    through most the data points to show the general direction of a time series

- seasonal component:
    cycle that occurs over a fixed period of time.

- residuals:
    what can not be explained by the trend or the seasonal components. we can
    think of it as, adding the trend ans seasonal graphs together and comparing
    the value at each point

    usually corresponds to random errors, white noise and so on.

    they represent info the model can not predict since it is random.


visualizing these components is known as _decomposition_


brid's-eye view of time series forecasting
------------------------------------------

forecasting: predicting the future using historical data and knowledge of
future events that might affect our forecasts.


1. set a goal
2. determine what must be forecast to achieve our goal
3. set the horizon of the forecast (horizon: period you want to predict the
data on)
4. gather the data
5. *develop a forecasting model*
6. deploy to production
7. monitor
8. collect new data
9. repeat step 5


diff between time series forecasting and other regressions tasks
----------------------------------------------------------------

- time series have an order
  - time series are indexed by time, order must be kept. on the other hand, say
    you want to predict revenue based on ad spend, it does not matter when
    certain amount was spent on ads.

-  time series do not have features
  - it is common to only have two columns, time and the data itself, it does
    not have categories and so on


baseline model
==============

a baseline model is a trivial solution to our problem. it uses heuristics or
simple statistics to generate predictions.

The baseline model is the simplest solution you can think of.

In the context of time series, simple static we can use to build baseline is
the arithmetic mean.

something like saying:

>>> the average EPS between 1960 and 1979 was $4.31. Therefore, I expect the
>>> EPS over the next four quarters of 1980 to be equal to $4.31 per quarter

other way is to naively forecast the last recorded data point.

>>> if the EPS is $0.71 for this quarter, then the EPS will also be $0.71 for
>>> next quarter

or if we see a cyclical pattern:

>> if the EPS is $14.04 for the first quarter of 1979, then the EPS for the
>> first quarter of 1980 will also be $14.04

when we define a baseline we need to calculate an error metric in order to
evaluate the performance of our forecasts on the test set

- mean absolute percentage error (MAPE):
    measure prediction accuracy, independent of the scale of our data

check the notebook: ch2/baseline.ipynb for more details


random walk
===========

>>> a random walk is a series whose first difference is stationary and
>>> uncorrelated.
>>>
>>> this means that the process moves completely random.

a random walk is a process in which there is an equal chance of going up or
down by a random number. Usually observed in financial and economic data.

Normally they also have long periods of positive or negative trends, but sudden
changes in direction.

a random walks is expressed by:
    the previous value plus white noise and some constant


how to find out if we have a random walk?
-----------------------------------------

1. gather data
2. is it stationary? if not make it (transformations)
3. plot ACF
4. is there autocorrelation?
    - no, random walk
    - yes, not a random walk

let's break down the concepts


stationarity
------------

>>> a stationary time series is one whose statistical properties do not change
>>> over time.
>>>
>>> It has constant mean, variance, and autocorrelation, and these properties
>>> are independent of time.

many forecasting models assume stationarity. moving average model,
autoregressive model, and autoregressive moving average model all assume
stationary.

we rarely see this stationary time series, this is when ARIMA and SARIMA come
into play.

There are several way to transform a time series to become stationary, the
simplest one is differencing. Calculate the changes from one step to another.
This transformation helps stabilize the mean. Which reduces the trend and
seasonality effects.

Differencing: stabilize the mean
Apply log function: stabilize the variance

make sure to inverse transform the data after finishing the model.

- how to test for stationarity?
Augmented Dickey-Fuller (ADF) test, if the p value is less than 0.05 we can say
the series is stationary

a stationary series has constant properties over time, mean and variance are
not a function of time. the mean of stationary process should be flat over time
the mean of non-stationary process should vary


autocorrelation function
------------------------

once the time series become stationary, we plot the autocorrelation function
(acf). It is a great way to understand what type of process you are analyzing.

correlation measures the extent of a linear relationship between two
variables.

>>> the autocorrelation function measures the linear relationship between
>>> lagged values of a time series. ACF measures the correlation of the time
>>> series with itself

in the presence of a trend, a plot of the ACF will show coefficients are high
for short lags, and will decrease as the lag increases. If the data is
seasonal, it will also display cyclical patterns.

check ch3/random_walk.ipynb for more details


basically if we are presented with a random walk, we cannot predict anything at
the long term with an statistical model.

we can only use methods like the one we use in the baseline part. (historical
mean, last known value)

nevertheless there is a method that we have not take into account:

the drift method
----------------

>>> States that the value of our forecast is linearly dependent on the timestep
>>>
>>> Therefore, it is equivalent to calculating the slope between the first and
>>> last value of the train set and simply extrapolating this straight line
>>> into the future.

basically getting the slope and calculating the values based on that.

slope = (delta y) / (delta x)
      = (yf - yi) / #timesteps - 1

check ch3/random_walk.ipynb for more details

we cannot forecast long term a random walk.


forecasting next timestep
-------------------------

since the value of a random walk is the past value by some random number, if we
take the last value we will be off just by that random number

this might look deceiving to untrained eyes, because even the MSE is low. But
remember we are just taking the last known value at each timestep and inserting
it in the next.

the conclusion is, if we find a random walk, we cannot use deep learning or
statistical models to predict future values.


moving average process
======================

>>> In a moving average (MA) process, the current value depends linearly on the
>>> mean of the series, the current error term, and past error terms.

It is usually denoted as MA(q) where q is the order (the number of past error
terms that affect the present value).

yt = µ + εt + (θ1 (ε_(t-1)) + (θ2 (ε_(t-2)) + ... + (θq (ε_(t-q))

where:
- µ is the mean
- εt error term
- ε_(t - k)  past error term
- θ magnitude of impact of the past errors

The larger q is, the more past error terms affect the present value.

This means that we need to choose this number carefully in order to train the
data correctly

1. gather data
2. is it stationary? if not make it (transformations)
3. plot ACF
4. is there autocorrelation?
    - no, random walk
5. yes. Do autocorrelation coefficients become abruptly non-significant after
   lag q?
   - yes, MA(q) process
   - no, not MA(q) process

we get q from the ACF plot, when we see that the correlation ends.

see ch4_moving_average/moving_average_order_q.ipynb for more details

Now, say q = 2, meaning we will only take 2 past errors to calc each value. We
can not take this and calculate 50 timesteps into the future. Because we don't
have the 2 past error for future values.

What we can do is called _rolling forecasting_ which basically is iterate the
training, and based on new results obtain values further in the future

say we have 10 steps, and a q of 2. The first iteration we can predict until
12, the second one we will take the new values and train with those so we will
be able to go until 14.

see ch4_moving_average/moving_average_order_q.ipynb for more details

we can use the mean absolute error, it returns the average of the absolute
difference between the predicted and actual values

again, for more details ch4_moving_average/moving_average_order_q.ipynb
there we have an interpretation.

autoregressive process
======================

an autoregressive process establishes that the output variable depends linearly
on its own previous states.

>>> An autoregressive process is a regression of a variable against itself. In
>>> a time series, this means that the present value is linearly dependent on
>>> its past values.

It is defined by AR(p) where p is the order it defines the number of past
values that affect the present value:

yt = C + (φ1 (y_(t-1)) + (φ2 (y_(t-2)) + ... + (φp (y_(t-p)) + εt

where

- φ autoregressive coefficients that measure the strength of the
  relationship between the current value and past values.
- εt error term
- ε_(t - k)  past error term

if p is 1 and φ1 is 1,

yt = C + (φ1 (y_(t-1)) + εt

we have a random walk, so a random walk is a special case of an autoregressive
process.

We can now add steps to our list of time series process:

1. gather data
2. is it stationary? if not make it (transformations)
3. plot ACF
4. is there autocorrelation?
    - no, random walk
5. yes. Do autocorrelation coefficients become abruptly non-significant after
   lag q?
   - yes, MA(q) process
6. no, plot PACF
7. Do coefficients become abruptly non-significant after lag p
   - yes, AR(p) process
   - no, not an AR(p) process

How does the ACF plot looks if we are dealing with an autoregressive process?

check the notebook: ch5_autoregressive/autoregressive.ipynb

>>> when the ACF plot of a stationary process exhibits a pattern of exponential
>>> decay, we probably have an autoregressive process in play.
>>>
>>> we must turn our attention to the partial autocorrelation function (PACF)
>>> plot. to get the order of the autoregression

what does the partial autocorrelation does?

>>> the partial autocorrelation measures the correlation between lagged values
>>> in a time series when we remove the influence of correlated lagged values
>>> in between.

check the notebook: ch5_autoregressive/autoregressive.ipynb


autoregressive moving average process (ARMA)
==========================================

what happens when we will plot the ACF function and find that:

- there are significant autocorrelation coefficients after lag 0, which means
  it is not a random walk.
- we observe that coefficients slowly decay. They do not become abruptly
  non-significant after a certain lag, which means that it is not a purely
  moving average process.
- we plot PACF and we notice a sinusoidal pattern, meaning the coefficients do
  not become abruptly non-significant after a certain lag

(meaning that we have done all steps covered in previous chapters)

Then, we must have a combination of autoregressive and moving average
processes, which is modeled with the:

ARMA(p, q) model

where:
- p being the order of the autoregressive process
- q the order of the moving average process

we used to calc these two numbers with ACF and PACF plot, but it will be hard
to get them from there in this case. That is when Akaike information criterion
comes into play

>>> The autoregressive moving average process is a combination of the
>>> autoregressive process and the moving average process.

>>> An ARMA(0,q) process is equivalent to an MA(q) process, since the order
>>> p = 0 cancels the AR(p) portion. An ARMA(p,0) process is equivalent to an
>>> AR(p) process, since the order q = 0 cancels the MA(q) portion.


yt = C + (φ1 (y_(t-1)) + (φ2 (y_(t-2)) + ... + (φp (y_(t-p)) µ + εt
     + (θ1 (ε_(t-1)) + (θ2 (ε_(t-2)) + ... + (θq (ε_(t-q))

It is kind of hard to see in plain text but it basically just the two formulas
(for AR and MA) added up.

where:
- p determines the number of past *values* that affect the present value.
- q determines the number of past *errors* that affect the present value.

The steps then, remain as follows:
1. gather data
2. is it stationary? if not make it (transformations)
3. plot ACF
4. is there autocorrelation?
    - no, random walk
5. yes. Do autocorrelation coefficients become abruptly non-significant after
   lag q?
   - yes, MA(q) process
6. no, plot PACF
7. Do coefficients become abruptly non-significant after lag p
   - yes, AR(p) process
   - no, it is an ARMA(p,q) process

>>> Identifying a stationary ARMA(p,q) process
>>>
>>> If your process is stationary and both the ACF and PACF plots show a
>>> decaying or sinusoidal pattern, then it is a stationary ARMA(p,q) process.

we know that we need to calc q and p in order to do anything, but ACF and PACF
plots wont tell us much, we need a new method, and say goodbye to our old
friends :(

1. gather data
2. is it stationary? if not make it (transformations)
3. list values of p and q
4. fit every combination of ARMA(p,q)
5. select model with lowest AIC
6. residual analysis
    - Q-Q plot shows a straight line? no? go back to step 3
    - Uncorrelated residuals? no? go back to step 3
7. if the answer to the last two question is yes, ready to forecasts

Akaike information criteria (AIC)
---------------------------------


>>> The AIC estimates the quality of a model relative to other models.
>>>
>>> Given that there will be some information lost when a model is fitted to
>>> the data, the AIC quantifies the information lost.
>>>
>>> The less information lost, the lower the AIC value and the better the
>>> model.

It is expressed by:

AIC = 2k - ln(L)

where:
- k is the number of estimated parameters
- L maximum value of the likelihood function model

Using the AIC to select our model, allows us to keep a balance between the
complexity of the model and its goodness of fitting the data

k is directly affected by the order of p and q in ARMA(p,q)
say,
p = 2
q = 2
then k = 2 + 2 = 4

the higher the order gets, the AIC increases, making it penalize more complex
models

L (the likelihood function) measures the goodness of fit of a model. It can be
seen as the opposite of the distribution function

- dist function: given a model with fixed params, measures the probability of
  observing a data point.

- likelihood function: given a set of data, it tell us how likely it is that
  diff model parameters will generate the data.

in case that was unclear, here is an example:

consider we will roll a six-sided die:

- the dist function tells us that there is a 1/6 probability that we'll observe
  [1,2,3,4,5,6]

- suppose that you roll a die 10 times and you obtain [1,5,3,4,6,2,4,3,2,1].
  The likelihood function will determine how likely is that the die has six
  sides.

so the question in our context is, "how likely is that my observed data is
coming from an ARMA(1,1)"

if a model fits the data really well, the maximum value of likelihood will be
high.

since AIC function subtracts the ln of L from 2k, it balance between
underfitting and overfitting.

AIC quantifies the quality of a model in relation to other models only.  so it
is a relative measure of quality.

What we end up doing in code is iterating thru some combinations of p and q, to
calc the AIC values and selecting the one with the lowest score

calculating residuals ---------------------

The residuals of a model are simply the difference between the predicted values
and the actual values.

we need to check two things

1. Q-Q plot 2. Ljung-Box test

if the process found by the arma model is the same as the original process,
then the residual would be just εt (the error term), meaning white noise, stuff
we cannot predict based on past values.

the first step is making the Q-Q plot:
- residuals are plotted in y-axis
- quanitles of theoretical dist (in this case normal) on the x axis

we want our error to behave as a normal distribution so we want the two lines
exactly the same `y = x`.

if we don't get this result we can say that our distribution of residuals does
not resemble a normal dist, so our model is not a good fit.

>>> A Q-Q plot is a plot of the quantiles of two distributions against each
>>> other. In time series forecasting, we plot the distribution of our
>>> residuals on the y-axis against the theoretical normal distribution on the
>>> x-axis.
>>>
>>> if our residuals is similar to a normal distribution, we will see a
>>> straight line lying on y = x. This means that our model is a good fit.

After having a good Q-Q plot we go to Ljung-Box test to demonstrate that the
residuals are uncorrelated.

>>> Remember that a good model has residuals that are similar to white noise,
>>> so the residuals should be normally distributed and uncorrelated.

Ljung-Box test

>>> The Ljung-Box test is a statistical test that determines whether the
>>> autocorrelation of a group of data is significantly different from 0.

>>> The null hypothesis states that the data is independently distributed,
>>> meaning that there is no autocorrelation. If the p-value is larger than
>>> 0.05, we cannot reject the null hypothesis meaning that the residuals are
>>> independently distributed.

so basically if in the Q-Q plot y = x and when doing the Ljung-Box  test we get
a p value larger than 0.05 we can start using the model for forecasting.

since in ARMA we still have the MA(q) component, we will only be able to use
forecast q periods into the future, otherwise we would only get the mean


non-stationary time series (ar integrated ma)
=============================================

basically we will add the integrated order so we can model in non-stationary
time series without having to model data transformed and making the inverse
transform to get the final outputs

so ARIMA(p,d,q) (autoregressive integrated moving average model) is a
combination of AR(p) and MA(q) but we don't have to transform the series before

d corresponds to the minimum number of times we had to do differencing in order
to make the series stationary.

in the experience of the author, a time series rarely needs to be differenced
more than twice to become stationary.

>>> It is denoted as ARIMA(p,d,q), where p is the order of the AR(p) process, d
>>> is the order of integration, and q is the order of the MA(q) process.

>>> Integration is the reverse of differencing, and the order of integration d
>>> is equal to the number of times the series has been differenced to be
>>> rendered stationary.

>>> A time series that can be rendered stationary by applying differencing is
>>> said to be an integrated series.

The equation is pretty similar:

yt = C + (φ1 (y'_(t-1)) + ... + (φp (y'_(t-p)) + (θ1 (ε'_(t-1)) + ... + (θq
(ε'_(t-q)) + εt

y't represents  the differenced series, it may have been differenced more than
once.

1. gather data 2. is it stationary? if not make it 3. set d to the number of
times the series was differenced 4. list values of p and q 5. fit every
combination of ARIMA(p,d,q) 6. select model with lowest AIC 7. residual
analysis
    - Q-Q plot shows a straight line? no? go back to step 3
    - Uncorrelated residuals? no? go back to step 3 8. if the answer to the
      last two question is yes, ready to forecasts


accounting for seasonality ==========================

we are going to talk about the SARMIA(p,d,q)(P,D,Q)m model, this model takes
into account seasonlity.

we notice new params P,D,Q and m, the first 3 are the same as their counter
parts in lowercase but for seasonality, but before explaining them properly we
need to shift our attention to the `m` param

m stands for frequency
>>> In the context of a time series, the frequency is defined as the number of
>>> observations per cycle.

the length of the cycle depends on the dataset

For data that was recorded every year, quarter, month, or week, the length of a
cycle is considered to be 1 year.

if the data is collected
- annual then frequency (m) is 1
- quarterly then frequency (m) is 4
- monthly then frequency (m) is 12
- weekly then frequency (m) is 52

When data is collected on a daily or sub-daily basis, there are multiple ways
of interpreting the frequency

| data collection | minute | hour | day   | week   | year     |
|-----------------|--------|------|-------|--------|----------| | daily
|        |      |       | 7      | 365      | | hourly          |        |
| 24    | 168    | 8766     | | every minute    |        | 60   | 1440  | 10080
| 525960   | | every second    | 60     | 3600 | 86400 | 604800 | 31557600 |


>>> The seasonal autoregressive integrated moving average (SARIMA) model adds
>>> seasonal parameters to the ARIMA(p,d,q) model.
>>>
>>> It is denoted as SARIMA(p,d,q)(P,D,Q)m, where P is the order of the
>>> seasonal AR(P) process, D is the seasonal order of integration, Q is the
>>> order of the seasonal MA(Q) process, and m is the frequency, or the number
>>> of observations per seasonal cycle.

Let's use an example, say we have a dataset of an airline's total monthly air
passengers, m would be 12, because we know that this is monthly data. If we see
that on August we have more flights, the information of that month in prior
years is likely going to be useful.

identify seasonal patterns in a time series
--------------------------------------------

we can use time series decomposition, which is a technique of breaking the time
series into three components:
- trend: long term change
- seasonability: repeated fluctuations that occur over a fixed period of time
- residuals: noise, express any irregularity that cannot be explained by the
  trend or the seasonal component.

go to the notebook on ch8* to see more info

you might be surprised -- like i was -- to find out that in fact, there are no
statistical tests to identify seasonality in time series.

so the steps are as follows:

1. gather data 2. is it stationary? if not make it 3. set d to the number of
times the series was differenced 4. set D to the number of times seasonal
differencing was applied 5. list values of p, q, P, Q.  6. fit every
combination of SARIMA(p,d,q)(P,D,Q)m 7. select model with lowest AIC 8.
residual analysis
    - Q-Q plot shows a straight line? no? go back to step 3
    - Uncorrelated residuals? no? go back to step 3 9. if the answer to the
      last two question is yes, ready to forecasts

exogenous sarima
================
we'll use sarimax to take into account external factors.

in statistics
- exogenous is used to describe predictors or input variables.
- endogenous is used to define the target variable. (what we are trying to
  predict)

>>> sarimax model adds a linear combination of exogenous variables to the
>>> sarima model.

y = SARIMA(p,d,q)(P,D,Q)m + for i to n: βiX^i_t

check the notebook for the formula, since it is quite difficult to write it in
plain text

Basically here we will have a dataframe with more than two columns. we will
likely have more categories that we will take as our external factor.

We don't have to do any feature selection since the linear model will attribute
a coefficient close to 0 for exogenous variables that are not significant in
predicting the target.

